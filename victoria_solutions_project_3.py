# -*- coding: utf-8 -*-
"""Victoria Solutions - Project 3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OlxsfiuXo2QVczewMpCHC8MdT8wwTgfr
"""
######################################################################    
# Setup and Import
######################################################################


# Importing Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import data_table
import math
from scipy import stats

# Additional libraries for Advanced Data Analysis
from sklearn.linear_model import LinearRegression, LogisticRegression  # Library for Predictive Modelling
from scipy.stats import f_oneway, zscore, norm  # Library for Statistical Analysis
from sklearn.cluster import KMeans  # Library for Machine Learning
from sklearn.preprocessing import LabelEncoder, StandardScaler
from statsmodels.tsa.arima.model import ARIMA
from sklearn.decomposition import FactorAnalysis
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from xgboost import XGBClassifier

# Extras Required
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix

# Importing Sales_Data

# Paths
data_path = "/content/drive/MyDrive/Colab Notebooks/Project 3 Sales Data - VS.xlsx"

# Load Data
Sales_Data = pd.read_excel(data_path)

# Enable interactive tables
data_table.enable_dataframe_formatter()

# Show Excel-like interactive table
Sales_Data

######################################################################
# 1. Sales_Data Preprocessing and Cleaning
######################################################################


# Handle Missing Values -> Ensures no gaps distort analysis.

# --- Test 1: Check Missing Values ---
missing_values = Sales_Data.isnull().sum().reset_index()
missing_values.columns = ["Column", "Missing Values"]
print("ðŸ”¹ Missing Values Check")
display(missing_values)

# Fill missing values
for col in Sales_Data.columns:
    if Sales_Data[col].dtype in ['int64', 'float64']:
        Sales_Data[col] = Sales_Data[col].fillna(Sales_Data[col].median())
    else:
        Sales_Data[col] = Sales_Data[col].fillna(Sales_Data[col].mode()[0])

# Remove Outliers -> Prevents unusual purchases from skewing results.
"""
- Process: We are checking for extreme values in numeric columns using the Z-score method.
- A Z-score above 3 indicates a potential outlier. The table above shows how many outliers were detected per column.
"""

# --- Test 2: Check Outliers (Z-score for numeric columns) ---
numeric_cols = Sales_Data.select_dtypes(include=[np.number])
z_scores = np.abs(stats.zscore(numeric_cols, nan_policy='omit'))

# Count outliers per column
outlier_counts = pd.Series((z_scores > 3).sum(axis=0), index=numeric_cols.columns).reset_index()
outlier_counts.columns = ["Column", "Outliers"]

print("ðŸ”¹ Outliers Check (Z-score > 3)")
display(outlier_counts)

# --- Remove outliers using Z-score ---
numeric_cols = Sales_Data.select_dtypes(include=[np.number])
z_scores = np.abs(stats.zscore(numeric_cols, nan_policy='omit'))
Sales_Data = Sales_Data[(z_scores < 3).all(axis=1)]

# --- IQR Analysis for Outlier Check ---

# Detailed IQR calculation for 'Total_Spend' column
total_spend_sorted = np.sort(Sales_Data['Total_Spend'].values)
Q1 = np.percentile(total_spend_sorted, 25)
Q3 = np.percentile(total_spend_sorted, 75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
min_value = total_spend_sorted.min()
max_value = total_spend_sorted.max()

# Create a summary DataFrame
iqr_summary = pd.DataFrame({
    "Metric": ["Sorted Values", "Q1", "Q3", "IQR", "Lower Bound", "Upper Bound", "Min Value", "Max Value", "Conclusion"],
    "Value": [
        total_spend_sorted.tolist(),
        Q1,
        Q3,
        IQR,
        lower_bound,
        upper_bound,
        min_value,
        max_value,
        "All values fall within bounds â†’ no outliers"
    ]
})

print("ðŸ”¹ IQR Analysis for 'Total_Spend'")
display(iqr_summary)

# --- Boxplot for visual proof ---
numeric_cols = Sales_Data.select_dtypes(include=[np.number]).drop(columns=['Customer ID'], errors='ignore')

num_cols = len(numeric_cols.columns)
cols_per_row = 3
rows = math.ceil(num_cols / cols_per_row)

# Set style for a Tableau-like professional look
sns.set_theme(style="whitegrid", palette="pastel")

fig, axes = plt.subplots(rows, cols_per_row, figsize=(cols_per_row * 4.5, rows * 4))
axes = axes.flatten()

for i, col in enumerate(numeric_cols.columns):
    sns.boxplot(
        x=Sales_Data[col],
        ax=axes[i],
        width=0.4,
        fliersize=3,        # smaller outliers
        linewidth=1.2       # cleaner borders
    )
    axes[i].set_title(f"{col}", fontsize=13, weight="bold", pad=10)
    axes[i].set_xlabel("")  # remove redundant x-label text
    axes[i].tick_params(axis="x", labelsize=10)

    # Remove unnecessary spines (like Tableau style)
    sns.despine(ax=axes[i])

# Turn off unused subplots
for j in range(i + 1, len(axes)):
    axes[j].axis("off")

plt.tight_layout(pad=2)
plt.show()

# Standardize Categorical Variables -> Ensures consistent labels for analysis.

# --- Test 3: Check inconsistent categorical variables ---
categorical_cols = Sales_Data.select_dtypes(include=["object"]).columns  # ONLY column names
inconsistent_cats = pd.DataFrame({
    "Column": categorical_cols,
    "Unique Values": [Sales_Data[col].unique().tolist() for col in categorical_cols]
})

print("ðŸ”¹ Categorical Variable Check")
display(inconsistent_cats)

# Example categorical encoding (update column names as needed)
le = LabelEncoder()

if 'Churned' in Sales_Data.columns:
    # Strip spaces and normalize case
    Sales_Data['Churned'] = Sales_Data['Churned'].astype(str).str.strip().str.lower()

    # Map to 1/0, keeping only recognized values
    mapping = {'yes': 1, 'no': 0}
    Sales_Data['Churned'] = Sales_Data['Churned'].map(mapping)

    # Check for any unexpected values
    unexpected = Sales_Data['Churned'].isnull().sum()
    if unexpected > 0:
        raise ValueError(f"Churned column has {unexpected} unexpected values that are neither 'Yes' nor 'No'. Please check the data.")

# Final Cleaned Data

# Assign cleaned table
Sales_Data_clean = Sales_Data

print("\nâœ… Sales_Data Preprocessing and Cleaning")

# Display the cleaned table (Excel-like in Colab)
Sales_Data_clean

######################################################################
# 2. Predictive Modeling
######################################################################

#2.1 Linear Regression: Predicting Sales
"""
- Uses **Marketing Spend** and **Seasonality Index** to forecast **Total Spend**.
- Business Value: Understand how promotions and seasonal factors impact sales.
"""

# Features and target
X = Sales_Data_clean[['Marketing_Spend', 'Seasonality_Index']]
y = Sales_Data_clean['Total_Spend']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Fit linear regression model and Predictions

# Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)

# Predictions
y_pred = lr.predict(X_test)

# Metrics 

# Metrics
r_squared = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
coefficients = lr.coef_
intercept = lr.intercept_

# Printing Table

# Create Excel-style tables
metrics_table = pd.DataFrame({
    "Metric": ["R-squared", "RMSE"],
    "Value": [r_squared, rmse]
})

coeff_table = pd.DataFrame({
    "Feature": ["Intercept", "Marketing_Spend", "Seasonality_Index"],
    "Coefficient": [intercept, coefficients[0], coefficients[1]]
})

# Display results in styled tables (like your Outliers Check screenshot)
print("=== Model Evaluation Metrics ===")
display(metrics_table)

print("=== Linear Regression Model Coefficients ===")
display(coeff_table)

######################################################################
# **2.2 Logistic Regression: Predicting Churn
######################################################################
"""
- Uses customer behavior features to classify **Churn vs No Churn**.
- Business Value: Identifies at-risk customers to reduce churn.
"""

# Features and target for churn analysis
X_churn = Sales_Data_clean[['Total_Spend', 'Purchase_Frequency', 'Marketing_Spend']]
y_churn = Sales_Data_clean['Churned']

# Train-test split
Xc_train, Xc_test, yc_train, yc_test = train_test_split(
    X_churn, y_churn, test_size=0.2, random_state=42
)

# Logistic Regression, Predictions and Accuracy

# Logistic Regression
logreg = LogisticRegression(max_iter=1000)
logreg.fit(Xc_train, yc_train)

# Predictions
yc_pred = logreg.predict(Xc_test)

# Accuracy
accuracy = accuracy_score(yc_test, yc_pred)

# Coefficients and intercept and Metrics

# Coefficients and intercept
coefficients = logreg.coef_[0]

# Metrics table
metrics_table = pd.DataFrame({
    "Metric": ["Accuracy"],
    "Value": [accuracy]
})

# Print results

# Coefficients table
coeff_table = pd.DataFrame({
    "Feature": [ "Total_Spend", "Purchase_Frequency", "Marketing_Spend"],
    "Coefficient": [ coefficients[0], coefficients[1], coefficients[2]]
})

# Print results in separate sections
print("=== Model Evaluation Metrics ===")
display(metrics_table)

print("=== Logistic Regression Model Coefficients ===")
display(coeff_table)

######################################################################
# 2.3 Time Series Forecasting (ARIMA)
######################################################################
"""
- Forecasts **future sales trends**.
- Business Value: Supports demand planning, inventory, and budgeting.
"""

# --- Step 1: Create Monthly Sales ---
# Each 4 rows = one "month" (proxy)
monthly_sales = Sales_Data_clean.groupby(Sales_Data_clean.index)['Total_Spend'].sum()

# --- Step 2: Trend Analysis using Linear Regression ---
X_time = np.arange(len(monthly_sales)).reshape(-1,1)  # Time proxy
y_sales = monthly_sales.values

trend_model = LinearRegression()
trend_model.fit(X_time, y_sales)
y_trend_pred = trend_model.predict(X_time)

trend_slope = trend_model.coef_[0]
trend_r2 = r2_score(y_sales, y_trend_pred)

# Create Excel-style table for trend metrics
trend_table = pd.DataFrame({
    "Metric": ["Trend Slope", "Trend R-squared"],
    "Value": [trend_slope, trend_r2]
})

# --- Step 3: Time Series Forecasting with ARIMA ---
model = ARIMA(monthly_sales, order=(1,1,1))
model_fit = model.fit()
forecast_steps = 3
forecast = model_fit.forecast(steps=forecast_steps)

# --- Step 4: Extend Trend Line to Forecast Horizon ---
X_time_extended = np.arange(len(monthly_sales) + forecast_steps).reshape(-1,1)
y_trend_extended = trend_model.predict(X_time_extended)

# Print result

# --- Step 5: Plot Trend Analysis with ARIMA Forecast ---
plt.figure(figsize=(10,6))

# Historical sales
plt.plot(X_time, y_sales, label="Historical Sales", marker="o", color="blue")

# Trend line (extended)
plt.plot(X_time_extended, y_trend_extended, label="Trend Line (Extended)", linestyle="--", color="red")

# ARIMA forecast
forecast_index = np.arange(len(monthly_sales), len(monthly_sales)+forecast_steps)
plt.plot(forecast_index, forecast, label="ARIMA Forecast", marker="x", color="orange")

# Labels and title
plt.xlabel("Time Proxy (Customer Order)")
plt.ylabel("Total Spend")
plt.title("Sales Trend and Forecast (Trend + ARIMA)")
plt.legend()
plt.grid(True)
plt.show()

# --- Step 6: Print Results ---
print("=== Trend Analysis Metrics ===")
display(trend_table)

print("=== ARIMA Model Forecast (Next 3 Periods) ===")
print(forecast)

######################################################################
# 3. Statistical Analysis
######################################################################


# 3.1 ANOVA: Regional Sales Comparison
"""
- Tests if sales differ significantly across regions.
"""

# --- Step 1: Prepare region-level data ---
region_sales = Sales_Data_clean.groupby("Region")["Total_Spend"].sum().reset_index()

# --- Step 2: Run ANOVA across regions ---
anova_result = f_oneway(
    Sales_Data_clean[Sales_Data_clean['Region']=="North"]["Total_Spend"],
    Sales_Data_clean[Sales_Data_clean['Region']=="South"]["Total_Spend"],
    Sales_Data_clean[Sales_Data_clean['Region']=="East"]["Total_Spend"],
    Sales_Data_clean[Sales_Data_clean['Region']=="West"]["Total_Spend"]
)

# Print Result Table

# --- Step 3: Create Excel-style result tables ---
# Regional performance summary
region_table = pd.DataFrame({
    "Region": region_sales["Region"],
    "Total Sales": region_sales["Total_Spend"]
})

# ANOVA results
anova_table = pd.DataFrame({
    "Metric": ["F-statistic", "p-value"],
    "Value": [anova_result.statistic, anova_result.pvalue]
})

# --- Step 4: Plot Sales by Region ---
plt.figure(figsize=(7,5))
plt.bar(region_sales["Region"], region_sales["Total_Spend"], color=["#4C72B0", "#55A868", "#C44E52", "#8172B2"])
plt.xlabel("Region")
plt.ylabel("Total Spend")
plt.title("Regional Sales Performance - ANOVA Results")
plt.show()

# --- Step 5: Print results ---
print("=== Regional Sales Performance ===")
display(region_table)

print("=== ANOVA Test Results ===")
display(anova_table)

sns.boxplot(x='Region', y='Total_Spend', data=Sales_Data_clean, palette='viridis')

# Set the title for the plot
plt.title("Regional Sales Performance â€“ ANOVA Comparison")

# Display the plot with the new colors
plt.show()

######################################################################
# 3.2 Hypothesis Testing: Impact of Promotions
######################################################################
"""
- Compares sales between high vs low marketing spend groups.

1: Segment Data
- We divide customers into two groups:
- High Marketing Spend: Customers whose Marketing_Spend is above the median.
- Low Marketing Spend: Customers whose Marketing_Spend is at or below the median.
"""

# Segmentation
high_marketing = Sales_Data_clean[Sales_Data_clean['Marketing_Spend'] > Sales_Data_clean['Marketing_Spend'].median()][['Customer_ID', 'Marketing_Spend', 'Total_Spend']]
low_marketing = Sales_Data_clean[Sales_Data_clean['Marketing_Spend'] <= Sales_Data_clean['Marketing_Spend'].median()][['Customer_ID', 'Marketing_Spend', 'Total_Spend']]

# Add a group label for clarity
high_marketing['Group'] = 'High Marketing Spend'
low_marketing['Group'] = 'Low Marketing Spend'

# Combine for display
hypothesis_table = pd.concat([high_marketing, low_marketing], ignore_index=True)

print("=== Hypothesis Testing: Segmented Sales Data ===")
display(hypothesis_table)

# 2: Hypothesis t-test

# Extract sales values
high_sales = high_marketing['Total_Spend']
low_sales = low_marketing['Total_Spend']

# Perform independent two-sample t-test
t_stat, p_val = stats.ttest_ind(high_sales, low_sales)

print("=== Hypothesis Test Results ===")
print(f"T-statistic: {t_stat:.4f}")
print(f"P-value: {p_val:.4f}")

# Optional: interpret
if p_val < 0.05:
    print("Result: Significant difference in sales between high and low marketing spend groups (reject H0).")
else:
    print("Result: No significant difference in sales between groups (fail to reject H0).")

# Calculate mean Total_Spend per group
mean_sales = hypothesis_table.groupby('Group')['Total_Spend'].mean().reset_index()

# Plot bar chart
plt.figure(figsize=(7,5))
sns.barplot(x='Group', y='Total_Spend', data=mean_sales, palette=["#4C72B0", "#55A868"])
plt.title('Impact of Marketing Spend on Total Sales')
plt.ylabel('Average Total Spend')
plt.xlabel('Customer Group')
plt.ylim(0, max(mean_sales['Total_Spend'])*1.1)  # Slightly higher for better visualization

# Add value labels on top of bars
for i, row in mean_sales.iterrows():
    plt.text(i, row['Total_Spend'] + max(mean_sales['Total_Spend'])*0.02, f"{row['Total_Spend']:.0f}",
             ha='center', fontsize=10)

plt.show()

######################################################################
# 3.3 Factor Analysis: Key Purchase Drivers
######################################################################
"""
- Identifies latent factors behind customer purchases.

1: Perform Factor Analysis
"""

# Select features for factor analysis
fa_features = ['Total_Spend','Purchase_Frequency','Marketing_Spend','Seasonality_Index']
fa_data = Sales_Data_clean[fa_features]

# Standardise
scaler = StandardScaler()
factors_data_scaled = scaler.fit_transform(fa_data)

# Apply Factor Analysis with 2 components
fa = FactorAnalysis(n_components=2, random_state=42)
fa.fit(factors_data_scaled)

# Factor loadings
factor_loadings = pd.DataFrame(
    fa.components_.T,
    columns=['Factor 1 - Customer Engagement', 'Factor 2 - Marketing Responsiveness'],
    index=fa_features
)

#Interpret factors
print(f"\nFactor 1 variants explained: {fa.noise_variance_[0]:.3f}")
print(f"Factor 2 variants explained: {fa.noise_variance_[1]:.3f}")

print("=== Factor Loadings Table ===")
display(factor_loadings)

# 2: Using Factor Scores per Customer for Further Analysis

# Get factor scores for each customer
factors = fa.transform(factors_data_scaled)   # âœ… transform to get scores

# Create a DataFrame of factor scores per customer
factor_scores_df = pd.DataFrame(factors, columns=['Factor 1 Score', 'Factor 2 Score'])
factor_scores_df['Customer_ID'] = Sales_Data_clean['Customer_ID'].values

print("=== Factor Scores per Customer ===")
display(factor_scores_df)

# Combine factor scores with Total_Spend for visualization
plot_df = factor_scores_df.merge(Sales_Data_clean[['Customer_ID','Total_Spend']], on='Customer_ID')

plt.figure(figsize=(8,6))

# Scatter plot with color gradient based on Total_Spend
scatter = plt.scatter(
    x=plot_df['Factor 1 Score'],
    y=plot_df['Factor 2 Score'],
    c=plot_df['Total_Spend'],
    cmap='viridis',
    s=100,
    edgecolor='k'
)

plt.xlabel('Factor 1 (Customer Engagement)')
plt.ylabel('Factor 2 (Marketing Responsiveness)')
plt.title('Factor Analysis: Customer Purchase Drivers')
plt.colorbar(scatter, label='Total Spend')

# Annotate each point with Customer_ID
for i, row in plot_df.iterrows():
    plt.text(row['Factor 1 Score']+0.02, row['Factor 2 Score']+0.02, str(row['Customer_ID']), fontsize=9)

plt.grid(True)
plt.show()

######################################################################
# 4. Customer Segmentation with Machine Learning
######################################################################

# 4.1 Decision Tree
"""
- Provides simple rules to classify churn risk.
"""

# --- Step 1: Define features (purchasing behaviour) and target (churn) ---
X_churn = Sales_Data_clean[['Total_Spend', 'Purchase_Frequency', 'Marketing_Spend', 'Seasonality_Index']]
y_churn = Sales_Data_clean['Churned']

# 2: Train Decision Tree

# --- Step 2: Train Decision Tree ---
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(X_churn, y_churn)

# Print Results

# --- Step 3: Create Excel-style table for feature importance ---
feature_importance = pd.DataFrame({
    "Feature": X_churn.columns,
    "Importance": dt.feature_importances_
}).sort_values(by="Importance", ascending=False)

# --- Step 4: Visualize the Decision Tree ---
plt.figure(figsize=(12,10))
plot_tree(dt, feature_names=X_churn.columns, class_names=["No Churn","Churn"], filled=True)
plt.show()

# --- Step 5: Print Results ---
print("=== Customer Segmentation Feature Importance ===")
display(feature_importance)

######################################################################
# 4.2 K-Means Clustering
######################################################################
"""
- Segments customers into spending groups.
"""

# Step 1: Run K-Means
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
Sales_Data_clean['Customer_Segment'] = kmeans.fit_predict(
    Sales_Data_clean[['Total_Spend','Purchase_Frequency']]
)

# Segment Summary Table

# --- Step 2: Calculate summary stats including churn ---
segment_summary = Sales_Data_clean.groupby('Customer_Segment').agg({
    'Total_Spend': 'mean',
    'Purchase_Frequency': 'mean',
    'Marketing_Spend': 'mean',
    'Churned': lambda x: (x == 1).mean() * 100,  # churn rate %
    'Customer_ID': 'count'
}).rename(columns={
    'Total_Spend': 'Avg Spend',
    'Purchase_Frequency': 'Avg Freq',
    'Marketing_Spend': 'Avg Marketing',
    'Churned': 'Churn Rate (%)',
    'Customer_ID': 'Count'
}).reset_index()

# --- Step 3: Map cluster labels based on Avg Spend ---
sorted_clusters = segment_summary.sort_values("Avg Spend").reset_index(drop=True)
cluster_mapping = {
    sorted_clusters.loc[0, "Customer_Segment"]: "Budget Conscious",
    sorted_clusters.loc[1, "Customer_Segment"]: "High Engagement",
    sorted_clusters.loc[2, "Customer_Segment"]: "Premium Customers"
}
Sales_Data_clean['Segment'] = Sales_Data_clean['Customer_Segment'].map(cluster_mapping)
segment_summary['Segment'] = segment_summary['Customer_Segment'].map(cluster_mapping)

# Final summary table
segment_table = segment_summary[['Segment','Count','Avg Spend','Avg Freq','Avg Marketing','Churn Rate (%)']]
print("=== Customer Segmentation Summary ===")
display(segment_table)

# Print Results

# --- Step 4: Visualization Layout ---
fig, axes = plt.subplots(
    2, 2, figsize=(14, 10),
    gridspec_kw={'wspace': 0.35, 'hspace': 0.45}
)
fig.suptitle("Customer Segmentation Analysis Overview", fontsize=16, fontweight='bold', y=0.98)

# Helper function: place labels safely inside bars
def add_bar_labels(ax, values, fmt, offset_ratio=0.95):
    for i, v in enumerate(values):
        ax.text(i, v * offset_ratio, fmt.format(v),
                ha='center', va='top', color='black', fontweight='bold')

# --- Pie chart: Segment distribution ---
axes[0,0].pie(
    segment_summary['Count'],
    labels=segment_summary['Segment'],
    autopct='%1.1f%%',
    colors=sns.color_palette("Set2")
)
axes[0,0].set_title(f"Customer Segment Distribution\n(Total: {segment_summary['Count'].sum()} customers)")

# --- Spending distribution ---
sns.barplot(
    ax=axes[0,1],
    x='Segment', y='Avg Spend',
    data=segment_summary, hue='Segment',
    palette="Set2", legend=False
)
axes[0,1].set_title("Average Spending by Segment")
axes[0,1].set_xlabel("Customer Segment")
axes[0,1].set_ylabel("Avg Spend (Â£)")
add_bar_labels(axes[0,1], segment_summary['Avg Spend'], "Â£{:.0f}")

# --- Churn rate distribution ---
sns.barplot(
    ax=axes[1,0],
    x='Segment', y='Churn Rate (%)',
    data=segment_summary, hue='Segment',
    palette="Set2", legend=False
)
axes[1,0].set_title("Churn Rate by Segment")
axes[1,0].set_xlabel("Customer Segment")
axes[1,0].set_ylabel("Churn Rate (%)")

# --- Marketing spend ---
sns.barplot(
    ax=axes[1,1],
    x='Segment', y='Avg Marketing',
    data=segment_summary, hue='Segment',
    palette="Set2", legend=False
)
axes[1,1].set_title("Average Marketing Investment by Segment")
axes[1,1].set_xlabel("Customer Segment")
axes[1,1].set_ylabel("Avg Marketing (Â£)")
add_bar_labels(axes[1,1], segment_summary['Avg Marketing'], "Â£{:.0f}")

# Adjust layout so thereâ€™s no giant whitespace
plt.subplots_adjust(top=0.90, bottom=0.08, left=0.08, right=0.98, wspace=0.35, hspace=0.45)
plt.show()

# OPTIONAL FOR PRINTING K-CLUSTERING RESULTS:

# --- Side-by-Side Cluster Scatter Plots ---
fig, axes = plt.subplots(1, 2, figsize=(16, 7))
fig.suptitle("K-Means Clustering: Customer Segments", fontsize=16, fontweight='bold', y=1.02)

# --- Plot 1: Total Spend vs Purchase Frequency ---
scatter1 = axes[0].scatter(
    Sales_Data_clean['Total_Spend'],
    Sales_Data_clean['Purchase_Frequency'],
    c=Sales_Data_clean['Customer_Segment'],
    cmap='Set2',
    s=100,
    edgecolor='k'
)
axes[0].set_xlabel('Total Spend')
axes[0].set_ylabel('Purchase Frequency')
axes[0].set_title('Total Spend vs Purchase Frequency')
axes[0].grid(True)
cbar1 = fig.colorbar(scatter1, ax=axes[0], ticks=[0,1,2])
cbar1.set_label('Cluster')

# --- Plot 2: Total Spend vs Marketing Spend ---
scatter2 = axes[1].scatter(
    Sales_Data_clean['Total_Spend'],
    Sales_Data_clean['Marketing_Spend'],
    c=Sales_Data_clean['Customer_Segment'],
    cmap='Set2',
    s=100,
    edgecolor='k'
)
axes[1].set_xlabel('Total Spend')
axes[1].set_ylabel('Marketing Spend')
axes[1].set_title('Total Spend vs Marketing Spend')
axes[1].grid(True)
cbar2 = fig.colorbar(scatter2, ax=axes[1], ticks=[0,1,2])
cbar2.set_label('Cluster')

plt.tight_layout()
plt.show()

######################################################################
# 4.3 Random Forest (Ensemble Learning)
######################################################################
"""
- Improves churn prediction accuracy with multiple decision trees.
"""

# Features and Target
X = Sales_Data_clean[['Total_Spend', 'Purchase_Frequency', 'Marketing_Spend', 'Seasonality_Index']]
y = Sales_Data_clean['Churned']

# Train-Test Split
Xc_train, Xc_test, yc_train, yc_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Ensemble Models
"""
- Random Forest: Works by combining multiple decision trees to reduce overfitting and improve generalization. Good for datasets with mixed data types.
- XGBoost: Gradient boosting that optimizes errors sequentially; often achieves higher accuracy on structured/tabular data.
"""

# Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(Xc_train, yc_train)
rf_acc = accuracy_score(yc_test, rf.predict(Xc_test))

# XGBoost
xgb = XGBClassifier(eval_metric='logloss', random_state=42)
xgb.fit(Xc_train, yc_train)
xgb_acc = accuracy_score(yc_test, xgb.predict(Xc_test))

# Combine results in a table
results = pd.DataFrame({
    "Model": ["Random Forest", "XGBoost"],
    "Accuracy": [rf_acc, xgb_acc]
})

# Display as Excel-style table
from tabulate import tabulate
print(tabulate(results, headers='keys', tablefmt='fancy_grid'))

# Ensemble Voting for Enhanced Prediction Accuracy
"""
- We can combine predictions from both models using hard voting for a final prediction.
"""

voting_clf = VotingClassifier(
    estimators=[('rf', rf), ('xgb', xgb)],
    voting='hard'
)
voting_clf.fit(Xc_train, yc_train)
voting_preds = voting_clf.predict(Xc_test)
voting_acc = accuracy_score(yc_test, voting_preds)

# Print Result"""

# Results Table (Excel-like)
results = pd.DataFrame({
    'Model': ['Random Forest', 'XGBoost', 'Voting Ensemble'],
    'Accuracy': [rf_acc, xgb_acc, voting_acc]
})

print(tabulate(results, headers='keys', tablefmt='fancy_grid'))

# Accuracy Comparison Chart
plt.figure(figsize=(8,5))
plt.bar(results['Model'], results['Accuracy'], color=['skyblue', 'lightgreen', 'salmon'])
plt.ylim(0,1)
plt.title('Model Accuracy Comparison')
plt.ylabel('Accuracy')
plt.show()